{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting your hands dirty with LDA (Using the Foursquare API)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First things first, we want to develop a classifier that can detect if a user comment is misapplied to the venue. Think of all the times you've gone to an Amazon item and saw reviews that looked like the image below, \"Used but in very good condition\", \"fast shipping\" are these relevant to a product review? Or are they more appropriate for seller reviews? \n",
    "\n",
    "We're going to see if we can develop a comment classifier to detect which kinds of comments may be more appropriate for which venue type, and thus provide some indication on the \"appropriateness\" of the comment that can be used for further review, thus (we hope), improving the overall quality of Foursquare as a reviews platform\n",
    "\n",
    "As an example: \"This place got me drunk\" would be a comment you'd expect to see for a store category \"bar\" \n",
    "Yet, \"I want to have my wedding here\" is definitely not a comment you'd expect to see at a \"bar\" category, more likely you'd see such a category associated with a park, or other such venue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "RjovdGVzdC9CYWQgQW1hem9uIFJldmlld3MuUE5H\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image('F:/test/Bad Amazon Reviews.PNG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the previously built table-creator on a few categories of Foursquare venues. \n",
    "\n",
    "Find the category list code in the following page: https://developer.foursquare.com/categorytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named foursquare",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-b76adb25b2fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfoursquare\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0municodedata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named foursquare"
     ]
    }
   ],
   "source": [
    "import foursquare\n",
    "import json\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "\n",
    "CLIENT_ID = 'VSLROL5XMTRYZK3RJSESVVMSMAV3WN1LJF1BEW2RIJCDQOTY'\n",
    "CLIENT_SECRET = 'GKNMUYT1MLLJUBUZDG4KQ14YOMNZZBBIUAZBVGSBYYPAMKDX'\n",
    "client = foursquare.Foursquare(client_id=CLIENT_ID, client_secret=CLIENT_SECRET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all of your venues into a dataframe. Make sure that you have at least 4-6 categories of venues included in your data so we can seperate them out later with LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d5a233b3a5df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstart_cats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvenues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'limit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'near'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Seattle, WA'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'radius'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'5000'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'categoryId'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdicty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "start_cats = ['4bf58dd8d48988d130941735', '4d4b7105d754a06374d81259', '4d4b7105d754a06375d81259', '4d4b7104d754a06370d81259']\n",
    "venue_ids = []\n",
    "categories = []\n",
    "\n",
    "for cat in start_cats:\n",
    "    temp = client.venues.search(params={'limit':50, 'near': 'Seattle, WA', 'radius':'5000', 'categoryId': cat})\n",
    "    x = json.dumps(temp, indent = 4)\n",
    "    dicty = json.loads(x)\n",
    "    for venue in dicty['venues']:\n",
    "        venue_ids.append(venue['id'])\n",
    "        categories.append(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing features using string parsing and elementary text mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a normalizer function that will take the comments, lowercase all uppercase letters, split each comment into an array/list/vector of words, and remove stop words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import codecs\n",
    "from sklearn import feature_extraction\n",
    "# import mpld3            \n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "\n",
    "\n",
    "comments = []\n",
    "count = 0\n",
    "final_cats = []\n",
    "\n",
    "for ID in venue_ids:\n",
    "    venue_details = client.venues(VENUE_ID = ID)\n",
    "    path_to_text = venue_details['venue']['tips']['groups'][0]['items']\n",
    "    temp = []\n",
    "    for comment in path_to_text:\n",
    "        final_cats.append(categories[count])\n",
    "        comments.append(comment['text'])\n",
    "    count += 1\n",
    "\n",
    "print len(final_cats)\n",
    "print len(comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a normalizing method, one that will convert all letters in a string into lower case, and remove stopwords. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Count Vectorizer Does This"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the normalizer that you created to build a 'stripped' down comments array/vector/list (however you did it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CountVectorizer does this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize your feature set into a dense matrix representation using Countvectorizer from sklearn\n",
    "\n",
    "A helpful reference: http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "test = df[df['category'] != 0]\n",
    "\n",
    "v = CountVectorizer(stop_words='english', \n",
    "                      token_pattern='[a-zA-Z]+', \n",
    "                      max_df=0.5, min_df=2)\n",
    "X = pd.DataFrame(v.fit_transform(test['comments']).todense())\n",
    "X.columns = v.get_feature_names()\n",
    "y = test['category']\n",
    "\n",
    "# Lots of data so lets select some of the more important stuff\n",
    "kbest = SelectKBest(k=400)\n",
    "kbest.fit_transform(X, y)\n",
    "\n",
    "# boolean mask to get column names\n",
    "mask = kbest.get_support()\n",
    "voc = pd.DataFrame(np.asarray(v.get_feature_names())[mask])\n",
    "\n",
    "# Vectorize with the vocab from above\n",
    "v2 = CountVectorizer(vocabulary=voc[0])\n",
    "X = pd.DataFrame(v2.fit_transform(test['comments']).todense())\n",
    "X.columns = v2.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing the LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct your LDA with the pieces above by fitting the model to your feature set and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "lda = LDA(n_components=4)\n",
    "# This gives us new features, components.\n",
    "components = lda.fit_transform(X,y)\n",
    "print lda.score(X, y)\n",
    "y_pred = lda.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.values, y, stratify=y, test_size=0.30, random_state=42)\n",
    "\n",
    "lda = LDA(n_components=4)\n",
    "components_2 = lda.fit_transform(X_train,y_train)\n",
    "print lda.score(X_test, y_test)\n",
    "y_pred = lda.predict(X_test)\n",
    "print classification_report(y_test, y_pred)\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now print the LDA and at least a few of the categories on a 2-D discriminant chart. Do you think the method is seperating the categories very well? Should we tune it more? Would it be a good indicator of \"inappropriate commenting\"? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "X_new = lda.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.pairplot(plot_df, hue='y')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
